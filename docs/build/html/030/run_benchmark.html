

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Run Benchmark &mdash; RDB2G-Bench 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Random Search" href="random_search.html" />
    <link rel="prev" title="Micro Actions" href="micro_action.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            RDB2G-Bench
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../010/install_and_setup.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../010/tutorial.html">Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../020/download_precomputed.html">Download Pre-computed Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../020/reproduce_datasets.html">Reproduce Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmarks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="micro_action.html">Micro Actions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Run Benchmark</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-rdb2g_bench.benchmark.bench_runner">Benchmark Runner Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rdb2g_bench.benchmark.bench_runner.run_benchmark"><code class="docutils literal notranslate"><span class="pre">run_benchmark()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-rdb2g_bench.benchmark.benchmark">Core Benchmark Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rdb2g_bench.benchmark.benchmark.full_graph_heuristic_analysis"><code class="docutils literal notranslate"><span class="pre">full_graph_heuristic_analysis()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rdb2g_bench.benchmark.benchmark.main"><code class="docutils literal notranslate"><span class="pre">main()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-usage">Example Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#basic-benchmark-execution">Basic Benchmark Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#specific-methods-comparison">Specific Methods Comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#available-methods">Available Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#results-structure">Results Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#output-files">Output Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="random_search.html">Random Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="greedy_search.html">Greedy Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="evolutionary_algorithm.html">Evolutionary Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_optimization.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm_baseline.html">LLM-based Baseline</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">RDB2G-Bench</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Run Benchmark</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/030/run_benchmark.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="run-benchmark">
<h1>Run Benchmark<a class="headerlink" href="#run-benchmark" title="Link to this heading"></a></h1>
<p>This module provides the main interface for running comprehensive benchmarks on RDB2G-Bench.
The benchmark runner executes multiple neural architecture search algorithms and provides
statistical analysis and comparison across different methods, datasets, and tasks.</p>
<p>The benchmark system supports all available search strategies and automatically handles
data preparation, caching, multi-run execution, and results aggregation.</p>
<section id="module-rdb2g_bench.benchmark.bench_runner">
<span id="benchmark-runner-interface"></span><h2>Benchmark Runner Interface<a class="headerlink" href="#module-rdb2g_bench.benchmark.bench_runner" title="Link to this heading"></a></h2>
<p>RDB2G-Bench Benchmark Runner Module</p>
<p>This module provides the main entry point for running comprehensive benchmarks on RDB2G-Bench.
It offers a simplified interface to execute multiple neural architecture search algorithms
and compare their performance across different datasets and tasks.</p>
<p>The benchmark runner supports multiple search strategies including evolutionary algorithms,
greedy search, reinforcement learning, Bayesian optimization, and random sampling baselines.
Results are automatically aggregated and analyzed for statistical comparison.</p>
<dl class="py function">
<dt class="sig sig-object py" id="rdb2g_bench.benchmark.bench_runner.run_benchmark">
<span class="sig-prename descclassname"><span class="pre">rdb2g_bench.benchmark.bench_runner.</span></span><span class="sig-name descname"><span class="pre">run_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'rel-f1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'driver-top3'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">budget_percentage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_runs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a></span></span><a class="headerlink" href="#rdb2g_bench.benchmark.bench_runner.run_benchmark" title="Link to this definition"></a></dt>
<dd><p>Execute comprehensive benchmark analysis on specified dataset and task.</p>
<p>This function provides a high-level interface for running neural architecture search
benchmarks on RDB2G-Bench. It supports multiple optimization methods and automatically
handles data preparation, model training, evaluation, and results aggregation across
multiple runs for statistical robustness.</p>
<p>The benchmark process includes:
1. Dataset and task preparation with proper caching
2. Search space initialization with micro actions
3. Performance prediction dataset setup
4. Multiple independent runs with different random seeds
5. Statistical analysis and results aggregation
6. Trajectory analysis and CSV export for visualization</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>dataset (str): Name of the RelBench dataset to benchmark on.</dt><dd><p>Available datasets include “rel-f1”, “rel-avito”, “rel-amazon”, etc.
Defaults to “rel-f1”.</p>
</dd>
<dt>task (str): Name of the RelBench task to evaluate.</dt><dd><p>Task names depend on the dataset (e.g., “driver-top3”, “user-ad-visit”).
Defaults to “driver-top3”.</p>
</dd>
<dt>budget_percentage (float): Budget percentage for search algorithms as fraction</dt><dd><p>of total search space (0.0-1.0). Higher values allow more thorough search
but increase computational cost. Defaults to 0.05 (5%).</p>
</dd>
<dt>method (Union[str, List[str]]): Search method(s) to benchmark.</dt><dd><p>Options: “all”, “ea”, “greedy”, “rl”, “bo”, “random”, or list of methods.
“all” runs all available methods. Defaults to “all”.</p>
</dd>
<dt>num_runs (int): Number of independent runs for statistical analysis.</dt><dd><p>More runs provide better statistical confidence but increase runtime.
Defaults to 10.</p>
</dd>
<dt>seed (int): Base random seed for reproducibility. Each run uses seed + run_index.</dt><dd><p>Defaults to 0.</p>
</dd>
<dt><a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs: Additional configuration parameters:</dt><dd><ul class="simple">
<li><p>tag (str): Experiment tag for result organization. Defaults to “hf”.</p></li>
<li><p>cache_dir (str): Directory for caching datasets and models. 
Defaults to “~/.cache/relbench_examples”.</p></li>
<li><p>result_dir (str): Root directory for saving results and trajectories.
Defaults to “./results”.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>Dict: Comprehensive benchmark results containing:</dt><dd><ul class="simple">
<li><p>Method-wise aggregated statistics across all runs</p></li>
<li><p>Performance trajectories and convergence analysis</p></li>
<li><p>Statistical comparisons and rankings</p></li>
<li><p>Timing and efficiency metrics</p></li>
<li><p>Best architecture configurations found</p></li>
</ul>
</dd>
<dt>Each method entry includes:</dt><dd><ul class="simple">
<li><p>avg_actual_y_perf_of_selected: Average performance across runs</p></li>
<li><p>avg_rank_position_overall: Average ranking position</p></li>
<li><p>avg_percentile_overall: Average percentile ranking</p></li>
<li><p>selected_graph_ids_runs: Graph IDs selected in each run</p></li>
<li><p>avg_evaluation_time: Average time spent on evaluations</p></li>
<li><p>avg_run_time: Average total runtime per run</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Raises:</dt><dd><p>ValueError: If unsupported dataset, task, or method specified
FileNotFoundError: If required data files cannot be found or downloaded
RuntimeError: If benchmark execution fails due to system constraints</p>
</dd>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run all methods on default dataset/task</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">run_benchmark</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;rel-f1&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;driver-top3&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">budget_percentage</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print aggregated results</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">stats</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="s1">&#39;avg_actual_y_perf_of_selected&#39;</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">:</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;avg_actual_y_perf_of_selected&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run specific methods with custom configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">run_benchmark</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;rel-avito&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;user-ad-visit&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">budget_percentage</span><span class="o">=</span><span class="mf">0.10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">method</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ea&quot;</span><span class="p">,</span> <span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="s2">&quot;rl&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">num_runs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;custom_experiment&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cache_dir</span><span class="o">=</span><span class="s2">&quot;/custom/cache&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">result_dir</span><span class="o">=</span><span class="s2">&quot;/custom/results&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run quick test with single method</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">run_benchmark</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">budget_percentage</span><span class="o">=</span><span class="mf">0.05</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Note:</dt><dd><ul class="simple">
<li><p>Results are automatically saved to CSV files for further analysis</p></li>
<li><p>Performance trajectories are exported for visualization</p></li>
<li><p>All intermediate data is cached to speed up repeated experiments</p></li>
<li><p>GPU acceleration is used automatically when available</p></li>
<li><p>Large datasets may require significant memory and storage</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-rdb2g_bench.benchmark.benchmark">
<span id="core-benchmark-engine"></span><h2>Core Benchmark Engine<a class="headerlink" href="#module-rdb2g_bench.benchmark.benchmark" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="rdb2g_bench.benchmark.benchmark.full_graph_heuristic_analysis">
<span class="sig-prename descclassname"><span class="pre">rdb2g_bench.benchmark.benchmark.</span></span><span class="sig-name descname"><span class="pre">full_graph_heuristic_analysis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PerformancePredictionDataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overall_actual_y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">higher_is_better</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">method_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'Full</span> <span class="pre">Graph'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rdb2g_bench.benchmark.benchmark.full_graph_heuristic_analysis" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="rdb2g_bench.benchmark.benchmark.main">
<span class="sig-prename descclassname"><span class="pre">rdb2g_bench.benchmark.benchmark.</span></span><span class="sig-name descname"><span class="pre">main</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#rdb2g_bench.benchmark.benchmark.main" title="Link to this definition"></a></dt>
<dd></dd></dl>

<section id="example-usage">
<h3>Example Usage<a class="headerlink" href="#example-usage" title="Link to this heading"></a></h3>
<section id="basic-benchmark-execution">
<h4>Basic Benchmark Execution<a class="headerlink" href="#basic-benchmark-execution" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rdb2g_bench.benchmark.bench_runner</span><span class="w"> </span><span class="kn">import</span> <span class="n">run_benchmark</span>

<span class="c1"># Run all methods on default dataset and task</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">run_benchmark</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;rel-f1&quot;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;driver-top3&quot;</span><span class="p">,</span>
    <span class="n">budget_percentage</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Print summary of results</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">stats</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="s1">&#39;avg_actual_y_perf_of_selected&#39;</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">:</span>
        <span class="n">perf</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;avg_actual_y_perf_of_selected&#39;</span><span class="p">]</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;avg_rank_position_overall&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">: Performance=</span><span class="si">{</span><span class="n">perf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="specific-methods-comparison">
<h4>Specific Methods Comparison<a class="headerlink" href="#specific-methods-comparison" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare evolutionary algorithm vs greedy search</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">run_benchmark</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;rel-avito&quot;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;user-ad-visit&quot;</span><span class="p">,</span>
    <span class="n">budget_percentage</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ea&quot;</span><span class="p">,</span> <span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">],</span>
    <span class="n">num_runs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>

<span class="c1"># Extract key metrics</span>
<span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ea&quot;</span><span class="p">,</span> <span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">method</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> Results:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Average Performance: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;avg_actual_y_perf_of_selected&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Average Rank: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;avg_rank_position_overall&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Average Runtime: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;avg_run_time&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="available-methods">
<h2>Available Methods<a class="headerlink" href="#available-methods" title="Link to this heading"></a></h2>
<p>The benchmark runner supports the following search methods:</p>
<ul class="simple">
<li><p><strong>“all”</strong>: Run all available methods</p></li>
<li><p><strong>“ea”</strong>: Evolutionary Algorithm baseline</p></li>
<li><p><strong>“greedy”</strong>: Greedy Search strategies (forward, backward, random)</p></li>
<li><p><strong>“rl”</strong>: Reinforcement Learning with policy gradients</p></li>
<li><p><strong>“bo”</strong>: Bayesian Optimization with surrogate models</p></li>
<li><p><strong>“random”</strong>: Random sampling baseline</p></li>
</ul>
<p>You can specify a single method as a string or multiple methods as a list.</p>
</section>
<section id="results-structure">
<h2>Results Structure<a class="headerlink" href="#results-structure" title="Link to this heading"></a></h2>
<p>The returned results dictionary contains method-wise statistics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;Method Name&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;avg_actual_y_perf_of_selected&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>      <span class="c1"># Average performance</span>
        <span class="s2">&quot;avg_rank_position_overall&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>          <span class="c1"># Average ranking</span>
        <span class="s2">&quot;avg_percentile_overall&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>             <span class="c1"># Average percentile</span>
        <span class="s2">&quot;total_samples_overall&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>                <span class="c1"># Total architectures</span>
        <span class="s2">&quot;selected_graph_ids_runs&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>             <span class="c1"># Selected graph IDs</span>
        <span class="s2">&quot;avg_selection_metric_value&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>         <span class="c1"># Average selection metric</span>
        <span class="s2">&quot;selected_graph_origins&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>              <span class="c1"># Method origins</span>
        <span class="s2">&quot;avg_evaluation_time&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>                <span class="c1"># Average evaluation time</span>
        <span class="s2">&quot;avg_run_time&quot;</span><span class="p">:</span> <span class="nb">float</span>                        <span class="c1"># Average total runtime</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="output-files">
<h2>Output Files<a class="headerlink" href="#output-files" title="Link to this heading"></a></h2>
<p>The benchmark automatically generates several output files:</p>
<ul class="simple">
<li><p><strong>Individual Trajectories</strong>: <code class="docutils literal notranslate"><span class="pre">avg_trajectory_{method}_{num_runs}runs.csv</span></code></p></li>
<li><p><strong>Combined Trajectories</strong>: <code class="docutils literal notranslate"><span class="pre">all_methods_trajectories_{num_runs}runs.csv</span></code></p></li>
<li><p><strong>Performance Summary</strong>: <code class="docutils literal notranslate"><span class="pre">performance_summary_{num_runs}runs.csv</span></code></p></li>
</ul>
<p>These files are saved in: <code class="docutils literal notranslate"><span class="pre">{result_dir}/benchmark/{dataset}/{task}/{tag}/</span></code></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="micro_action.html" class="btn btn-neutral float-left" title="Micro Actions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="random_search.html" class="btn btn-neutral float-right" title="Random Search" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright KAIST Data Mining Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>